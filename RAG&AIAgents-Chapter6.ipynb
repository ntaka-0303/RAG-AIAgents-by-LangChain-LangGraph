{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1AqGfC8Dvu1LhYNtj9N7xVia8QeKy706P","authorship_tag":"ABX9TyM/gF/PVVovALRqUD1BXbyh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# 0) アンインストール\n","%pip -q uninstall -y \\\n","  langchain langchain-core langchain-community langchain-openai \\\n","  langchain-text-splitters langchain-chroma langchain-classic \\\n","  langchain-graph-retriever chromadb tokenizers numpy || true\n","\n","# 1) インストール\n","# LangChain 本体＆プラグイン\n","# LangChain 0.3 系で統一\n","%pip -q install \\\n","  \"langchain==0.3.12\" \\\n","  \"langchain-community==0.3.12\" \\\n","  \"langchain-openai==0.2.8\" \\\n","  \"langchain-text-splitters==0.3.4\" \\\n","  \"langchain-chroma==0.1.4\"\n","\n","# Chroma は 0.5 系（GraphRetriever の Chroma アダプタが期待）\n","%pip -q install \"chromadb==0.5.23\"\n","\n","# GraphRetriever 本体（Chroma extras 付き）\n","%pip -q install \"langchain-graph-retriever[chroma]==0.8.0\"\n","\n","# Transformers の要件を満たす tokenizers\n","%pip -q install \"tokenizers==0.23.0\"\n","\n","# その他ユーティリティは現状のままでOK（必要なら）\n","%pip -q install -U tiktoken pypdf python-docx bs4 chardet numpy\n","%pip -q install -U rank_bm25 sudachipy sudachidict_full sudachidict_core\n","\n","# 2) 環境変数にOpenAI APIキー\n","import os\n","from google.colab import userdata\n","os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n","os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n","os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n","os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n","os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-book\""],"metadata":{"id":"kQEqumKYERQ7","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c0c32d52"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, glob, json\n","import pandas as pd\n","from typing import List, Dict, Any\n","from langchain_core.documents import Document\n","from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter"],"metadata":{"id":"aFrOpJSlPfdp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_DIR = \"/content/drive/MyDrive/Colab Notebooks/rag_week1\"  # ←ご指定のパス\n","DOCS_DIR = f\"{DATA_DIR}/docs\"\n","META_CSV = f\"{DATA_DIR}/metadata.csv\"   # または medata.csv（綴りミス想定も考慮）\n","QA_JSONL = f\"{DATA_DIR}/qa.jsonl\""],"metadata":{"id":"2ejhTewbxu3C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 1) metadata.csv / qa.jsonl / docs/*.md を読む\n","# =========================\n","\n","# 1-1) metadata.csv（綴りゆれ対策）\n","meta_path = META_CSV if os.path.exists(META_CSV) else f\"{DATA_DIR}/medata.csv\"\n","assert os.path.exists(meta_path), f\"metadata.csv (or medata.csv) not found under {DATA_DIR}\"\n","meta_df = pd.read_csv(meta_path)\n","\n","# ファイル名で突合するため末尾ファイル名を追加（CSVの path は別環境の絶対パスでもOK）\n","meta_df[\"file_name\"] = meta_df[\"path\"].apply(lambda p: os.path.basename(str(p)))\n","\n","# id/title/category/effective_date/confidentiality/department/product_type を想定\n","# 列名が大小文字違い等でも最低限拾えるよう軽く正規化\n","meta_df.columns = [c.strip() for c in meta_df.columns]\n","\n","# 1-2) qa.jsonl\n","assert os.path.exists(QA_JSONL), f\"qa.jsonl not found under {DATA_DIR}\"\n","qa_items: List[Dict[str, Any]] = []\n","with open(QA_JSONL, \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        qa_items.append(json.loads(line))\n","print(f\"Loaded QAs: {len(qa_items)}\")\n","\n","# 1-3) docs/*.md\n","assert os.path.isdir(DOCS_DIR), f\"docs dir not found: {DOCS_DIR}\"\n","doc_paths = sorted(glob.glob(os.path.join(DOCS_DIR, \"*.md\")))\n","assert len(doc_paths) > 0, f\"No .md files found in {DOCS_DIR}\"\n","print(f\"Found markdown docs: {len(doc_paths)}\")\n","\n","# 1-4) mdテキスト＋メタデータ組み立て\n","#     → CSVのファイル名（末尾）と docs のファイル名をキーに突合\n","meta_index = {row[\"file_name\"]: row for _, row in meta_df.iterrows()}\n","\n","md_texts: List[str] = []\n","md_metas: List[Dict[str, Any]] = []\n","\n","for p in doc_paths:\n","    fname = os.path.basename(p)\n","    row = meta_index.get(fname)\n","    # 基本メタ\n","    meta = {\"source\": p}\n","    if row is not None:\n","        for col in [\"id\", \"title\", \"category\", \"effective_date\", \"confidentiality\", \"department\", \"product_type\"]:\n","            if col in row and pd.notna(row[col]):\n","                meta[col] = row[col]\n","    # 本文読み込み\n","    with open(p, \"r\", encoding=\"utf-8\") as f:\n","        text = f.read()\n","    md_texts.append(text)\n","    md_metas.append(meta)\n","\n","print(\"Docs prepared:\", len(md_texts))"],"metadata":{"id":"p8oYre_oyHqa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 2) 分割（RC / Token / Markdownヘッダ）\n","# =========================\n","\n","# 2-1) 汎用：RecursiveCharacter\n","rc_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=800, chunk_overlap=120, add_start_index=True\n",")\n","rc_docs: List[Document] = rc_splitter.create_documents(md_texts, metadatas=md_metas)\n","\n","# 2-2) トークン境界安全（tiktoken由来）\n","tok_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n","    chunk_size=300, chunk_overlap=60\n",")\n","tok_docs: List[Document] = tok_splitter.create_documents(md_texts, metadatas=md_metas)\n","\n","# 2-3) Markdownの見出し分割（見出しメタを付与）\n","md_header_splitter = MarkdownHeaderTextSplitter(\n","    headers_to_split_on=[(\"#\", \"h1\"), (\"##\", \"h2\"), (\"###\", \"h3\")]\n",")\n","md_docs: List[Document] = []\n","for text, base_meta in zip(md_texts, md_metas):\n","    parts = md_header_splitter.split_text(text)  # List[Document] (見出しメタ付き)\n","    for d in parts:\n","        d.metadata = {**base_meta, **(d.metadata or {})}\n","    md_docs.extend(parts)\n","\n","print(\"RC chunks:\", len(rc_docs))\n","print(\"TOK chunks:\", len(tok_docs))\n","print(\"MD chunks:\", len(md_docs))\n","if md_docs:\n","    print(\"例：MDの1つ目のメタデータ:\", md_docs[0].metadata)"],"metadata":{"id":"RbsE01RcyLKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 4) ベースラインのベクトル化 & Retriever\n","# =========================\n","from langchain_openai import OpenAIEmbeddings # Import the class here\n","from langchain_chroma import Chroma # Ensure Chroma is also imported if not already\n","\n","emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # コスト重視ならsmall、品質重視なら-large\n","vs = Chroma(collection_name=\"wk1_rc_base\", embedding_function=emb)\n","_ = vs.add_documents(rc_docs)\n","baseline_retriever = vs.as_retriever(search_kwargs={\"k\": 5})\n","\n","# 動作確認\n","probe_q = \"サクラ短期国債ファンドの信託報酬\"\n","for i, d in enumerate(baseline_retriever.invoke(probe_q), 1):\n","    print(f\"[{i}] {d.page_content[:60]}...  id={d.metadata.get('id')} title={d.metadata.get('title')}\")"],"metadata":{"id":"sRvBrLEWzDSB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 5) QA評価（qa.jsonl に基づく）\n","#    - Source Hit@k: must_have_source_id が上位kに含まれるか\n","#    - Answer Hit@k: 期待解答文字列が上位kの本文に現れるか\n","#    - Noise: 上位kのうち「どちらにも当てはまらない」割合の平均\n","# =========================\n","def evaluate_with_qas(retriever, qas: List[Dict[str, Any]], k: int = 5):\n","    src_hits = 0\n","    ans_hits = 0\n","    total = 0\n","    noise_sum = 0.0\n","\n","    for qa in qas:\n","        q = qa[\"question\"]\n","        must_id = qa.get(\"must_have_source_id\")\n","        answers = qa.get(\"answers\", [])\n","\n","        docs = retriever.invoke(q) or []\n","        docs = list(docs)[:k]\n","\n","        # Source 判定\n","        src_hit = any(d.metadata.get(\"id\") == must_id for d in docs)\n","        src_hits += int(src_hit)\n","\n","        # Answer 判定（単純な部分一致／必要あれば正規表現や正規化に拡張）\n","        def doc_has_any_answer(doc) -> bool:\n","            content = (doc.page_content or \"\")\n","            return any(a in content for a in answers)\n","\n","        ans_hit = any(doc_has_any_answer(d) for d in docs)\n","        ans_hits += int(ans_hit)\n","\n","        # Noise = どちらにも当てはまらないドキュメント割合\n","        noise_flags = []\n","        for d in docs:\n","            bad = True\n","            if must_id and d.metadata.get(\"id\") == must_id:\n","                bad = False\n","            if any(a in (d.page_content or \"\") for a in answers):\n","                bad = False\n","            noise_flags.append(1 if bad else 0)\n","\n","        denom = max(1, len(docs))\n","        noise_sum += sum(noise_flags) / denom\n","        total += 1\n","\n","    return {\n","        \"SourceHit@k\": src_hits / max(1, total),\n","        \"AnswerHit@k\": ans_hits / max(1, total),\n","        \"Noise\": noise_sum / max(1, total),\n","        \"N\": total,\n","    }\n","\n","print(\"Baseline (RC) metrics:\", evaluate_with_qas(baseline_retriever, qa_items, k=5))"],"metadata":{"id":"5GVY9fydzaSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 6) 別分割（TokenSplit / MDHeader）で比較\n","# =========================\n","def build_retriever(docs, name):\n","    v = Chroma(\n","        collection_name=name,\n","        embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n","    )\n","    v.add_documents(docs)\n","    return v.as_retriever(search_kwargs={\"k\": 5})\n","\n","retr_tok = build_retriever(tok_docs, \"wk1_tok\")\n","retr_md  = build_retriever(md_docs,  \"wk1_md\") if md_docs else None\n","\n","print(\"TokenSplit metrics:\", evaluate_with_qas(retr_tok, qa_items, k=5))\n","if retr_md:\n","    print(\"MDHeader   metrics:\", evaluate_with_qas(retr_md, qa_items, k=5))\n","else:\n","    print(\"MDHeader   metrics: (N/A)\")"],"metadata":{"id":"4RfRq_3Azwld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 7) 日本語トークナイズと同義語・ヘッダ連結\n","# =========================\n","from sudachipy import dictionary, tokenizer\n","\n","# Sudachi初期化\n","_sudachi = dictionary.Dictionary().create()\n","_mode = tokenizer.Tokenizer.SplitMode.C\n","\n","# ドメイン用：同義語（必要に応じて追加）\n","SYNONYMS_JA = {\n","    \"信託報酬\": [\"運用 管理 費用\", \"マネジメント フィー\"],\n","    \"解約 申込 締切\": [\"カットオフ\", \"解約 締切\", \"申込 締切\"],\n","    \"販売 勧誘\": [\"勧誘\", \"電話 勧誘\", \"営業 電話\"],\n","    \"信託財産 留保額\": [\"留保額\"],\n","}\n","\n","STOP_POS = {\"助詞\", \"助動詞\", \"記号\"}\n","STOP_TOKENS = {\"する\", \"ある\", \"なる\", \"こと\", \"ため\", \"および\", \"により\", \"について\"}\n","\n","def ja_tokenize_advanced(text: str, add_bigrams: bool = True):\n","    \"\"\"Sudachiで分かち書き＋品詞フィルタ＋基本形化＋バイグラム\"\"\"\n","    terms = []\n","    for m in _sudachi.tokenize(text or \"\", _mode):\n","        pos = m.part_of_speech()[0]\n","        if pos in STOP_POS:\n","            continue\n","        w = m.normalized_form()\n","        if (not w) or (w in STOP_TOKENS):\n","            continue\n","        terms.append(w)\n","    if add_bigrams and len(terms) >= 2:\n","        terms += [f\"{a}_{b}\" for a, b in zip(terms, terms[1:])]\n","    return \" \".join(terms)\n","\n","def expand_with_synonyms(text: str) -> str:\n","    \"\"\"粗い同義語展開をテキストに追記（BM25は語一致が命）\"\"\"\n","    buf = [text]\n","    for base, syns in SYNONYMS_JA.items():\n","        if all(t not in text for t in [base] + syns):\n","            continue\n","        buf.extend(syns)\n","    return \"\\n\".join(buf)\n","\n","def attach_header_weight(content: str, meta: dict, title_boost: int = 3) -> str:\n","    \"\"\"title/category/product_type などを複製して“実質重み付け”\"\"\"\n","    headers = []\n","    for key in (\"title\", \"category\", \"product_type\", \"department\"):\n","        v = meta.get(key)\n","        if isinstance(v, str) and v.strip():\n","            headers.append(v)\n","    header_text = \" / \".join(headers)\n","    return content + (\" \" + header_text) * max(0, title_boost)\n"],"metadata":{"id":"0OhVuI4o0779"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 8) BM25 Retriverの構築とQuery用ラッパー作成\n","# =========================\n","from copy import deepcopy\n","from langchain_community.retrievers import BM25Retriever\n","\n","def build_bm25_retriever_ja(docs, k=10, title_boost=3):\n","    bm25_docs = []\n","    for d in docs:\n","        d2 = deepcopy(d)\n","        enriched = expand_with_synonyms(d2.page_content or \"\")\n","        enriched = attach_header_weight(enriched, d2.metadata or {}, title_boost=title_boost)\n","        d2.page_content = ja_tokenize_advanced(enriched)\n","        bm25_docs.append(d2)\n","    retr = BM25Retriever.from_documents(bm25_docs)  # rank_bm25 ベース\n","    retr.k = k\n","    return retr\n","\n","# BM25（改良版）を tok_docs から作成\n","bm25_retriever = build_bm25_retriever_ja(tok_docs, k=10, title_boost=3)"],"metadata":{"id":"L-jsnDce4Bq0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 8.5) BM25 用クエリ前処理ラッパ（BaseRetriever 継承）+ ID 付与ラッパ\n","# =========================\n","from typing import Any, List\n","from langchain_core.retrievers import BaseRetriever\n","from pydantic import Field\n","import hashlib, os\n","from copy import deepcopy\n","\n","def _ensure_id(doc):\n","    if getattr(doc, \"metadata\", None) is None:\n","        doc.metadata = {}\n","    if \"id\" not in doc.metadata or not doc.metadata[\"id\"]:\n","        src = doc.metadata.get(\"source\")\n","        if isinstance(src, str) and src:\n","            base = os.path.splitext(os.path.basename(src))[0]\n","            doc.metadata[\"id\"] = base\n","        else:\n","            h = hashlib.sha1((doc.page_content or \"\").encode(\"utf-8\")).hexdigest()[:12]\n","            doc.metadata[\"id\"] = f\"AUTO_{h}\"\n","    return doc\n","\n","def _retriever_call(ret, query):\n","    if hasattr(ret, \"get_relevant_documents\"):\n","        return ret.get_relevant_documents(query)\n","    # Runnable 準拠\n","    return ret.invoke(query)\n","\n","class BM25QueryWrappedRetriever(BaseRetriever):\n","    \"\"\"BM25 に渡す前にクエリを 同義語展開＋分かち書き\"\"\"\n","    bm25: Any = Field(...)  # langchain_community.retrievers.BM25Retriever\n","\n","    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Any]:\n","        qx = ja_tokenize_advanced(expand_with_synonyms(query))\n","        return _retriever_call(self.bm25, qx)\n","\n","class IdSafeRetriever(BaseRetriever):\n","    \"\"\"返却ドキュメントに必ず metadata['id'] を付与\"\"\"\n","    inner: BaseRetriever\n","\n","    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Any]:\n","        docs = _retriever_call(self.inner, query) or []\n","        out = []\n","        for d in docs:\n","            d2 = deepcopy(d)\n","            out.append(_ensure_id(d2))\n","        return out\n","\n","bm25_wrapped = BM25QueryWrappedRetriever(bm25=bm25_retriever)\n","bm25_idsafe  = IdSafeRetriever(inner=bm25_wrapped)\n","vec_idsafe   = IdSafeRetriever(inner=baseline_retriever)"],"metadata":{"id":"agtZdxZRAIh9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 9) ハイブリッド（RRF）を構築\n","# =========================\n","from langchain_classic.retrievers import EnsembleRetriever\n","hybrid_retriever = EnsembleRetriever(\n","    retrievers=[vec_idsafe, bm25_idsafe],\n","    weights=[0.8, 0.2],\n","    c=30,\n","    id_key=\"id\",\n",")\n","\n","# デバッグ：peek も idsafe を通す\n","def peek(ret, q, k=5):\n","    print(\"Q:\", q)\n","    docs = ret.invoke(q)[:k]\n","    for i, d in enumerate(docs, 1):\n","        print(f\"[{i}] id={d.metadata.get('id')} title={d.metadata.get('title')} | {d.page_content[:60]}...\")\n","\n","q1 = \"サクラ短期国債ファンドの信託報酬は？\"\n","print(\"— Vector —\"); peek(vec_idsafe, q1)\n","print(\"\\n— BM25  —\"); peek(bm25_idsafe, q1)\n","print(\"\\n— Hybrid—\"); peek(hybrid_retriever, q1)\n"],"metadata":{"id":"ZBq6XOxP4MHA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =========================\n","# 10) 改良版での評価\n","# =========================\n","print(\"BM25 only  :\", evaluate_with_qas(bm25_idsafe, qa_items, k=5))\n","print(\"Vector only:\", evaluate_with_qas(vec_idsafe,   qa_items, k=5))\n","print(\"Hybrid RRF :\", evaluate_with_qas(hybrid_retriever, qa_items, k=5))"],"metadata":{"id":"JjAqXreC5vxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Week3: LLM（クエリ拡張・HyDEで使用）\n","\n","from langchain_openai import ChatOpenAI\n","# 温度0で安定的に書き換えを生成（モデルは環境に合わせて調整可）\n","llm_rewrite = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n"],"metadata":{"id":"2rXW-ZGy7LAR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.retrievers import BaseRetriever\n","from pydantic import Field\n","from typing import List, Any\n","from copy import deepcopy\n","\n","# 共通: 行分割（箇条書きにも対応）\n","def _split_lines(text: str) -> List[str]:\n","    lines = []\n","    for raw in (text or \"\").splitlines():\n","        t = raw.strip()\n","        if not t:\n","            continue\n","        # 「- 」「・」「1. 」などを削る\n","        t = t.lstrip(\"-・*0123456789.　 \").strip()\n","        if t:\n","            lines.append(t)\n","    return list(dict.fromkeys(lines))  # 重複除去\n","\n","# 公式MultiQueryRetriever\n","class MultiQueryRetriever(BaseRetriever):\n","    base: BaseRetriever       = Field(...)\n","    llm:  Any                 = Field(...)\n","    n_queries: int            = Field(default=4)\n","\n","    _prompt = (\n","        \"あなたは検索クエリの言い換え生成器です。入力質問に対して、\"\n","        \"語彙・言い回し・キーワード展開を変えた検索向けクエリを{n}個、日本語で列挙してください。\"\n","        \"出力は各行1クエリのみ。\\n\\n質問: {q}\\n\"\n","    )\n","\n","    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Any]:\n","        # 言い換えクエリを生成\n","        prompt = self._prompt.format(q=query, n=self.n_queries)\n","        rewrite = self.llm.invoke(prompt).content\n","        variations = _split_lines(rewrite)[: self.n_queries]\n","        if not variations:\n","            variations = [query]\n","\n","        # 各クエリで検索 → 結果を順序維持でユニーク化\n","        bucket = []\n","        seen = set()\n","        for q in [query] + variations:   # 元の質問も含める\n","            docs = _retriever_call(self.base, q) or []\n","            for d in docs:\n","                key = (d.metadata.get(\"id\") or d.page_content[:80])\n","                if key in seen:\n","                    continue\n","                seen.add(key)\n","                bucket.append(d)\n","        return bucket\n","\n","# MultiQuery retriever を構築\n","def build_multiquery_retriever(base_retriever, llm, n_queries=5):\n","    \"\"\"\n","    常に MultiQueryCompatRetriever（フォールバック）を返す。\n","    公式 MultiQueryRetriever の from_llm は PromptTemplate/Runnable を要求し、\n","    LangChain のバージョン差でエラーになりやすいため、安定の互換実装を採用。\n","    \"\"\"\n","    return MultiQueryRetriever(base=base_retriever, llm=llm, n_queries=n_queries)\n","\n","# idsafe ラッパでメタデータ id を保証\n","multiq_retriever = build_multiquery_retriever(baseline_retriever, llm_rewrite, n_queries=5)\n","multiq_idsafe    = IdSafeRetriever(inner=multiq_retriever)\n"],"metadata":{"id":"ZlaiBIwfIslJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class HyDERetriever(BaseRetriever):\n","    base: BaseRetriever = Field(...)\n","    llm:  Any           = Field(...)\n","\n","    _prompt = (\n","        \"次の質問に対する簡潔な参考回答（事実ベースの要約）を、検索向けの文章として日本語で作成してください。\"\n","        \"列挙や記号は避け、平叙文で150〜300字程度。\\n\\n質問: {q}\\n\"\n","    )\n","\n","    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Any]:\n","        hypo = self.llm.invoke(self._prompt.format(q=query)).content\n","        # 仮想文書と元質問の両方で検索（結合）\n","        docs_h = _retriever_call(self.base, hypo) or []\n","        docs_q = _retriever_call(self.base, query) or []\n","        # 順序維持ユニーク\n","        out, seen = [], set()\n","        for d in list(docs_h) + list(docs_q):\n","            key = (d.metadata.get(\"id\") or d.page_content[:80])\n","            if key in seen:\n","                continue\n","            seen.add(key)\n","            out.append(d)\n","        return out\n","\n","hyde_retriever = HyDERetriever(base=baseline_retriever, llm=llm_rewrite)\n","hyde_idsafe    = IdSafeRetriever(inner=hyde_retriever)"],"metadata":{"id":"3cCzTQ-7k6on"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RewritingRetriever(BaseRetriever):\n","    base: BaseRetriever = Field(...)\n","    llm:  Any           = Field(...)\n","\n","    _prompt = (\n","        \"あなたは金融ドメインの検索クエリ最適化エージェントです。入力質問を、\"\n","        \"検索に強いキーワード列へ変換してください。重要語を保ちつつ、同義語や製品名、\"\n","        \"時間表現（例: 14:00/午後2時）などの表記ゆれもカバーしてください。\"\n","        \"出力は日本語、箇条書きや記号なし、単一行の検索文字列のみ。\\n\\n質問: {q}\\n\"\n","    )\n","\n","    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Any]:\n","        rewritten = self.llm.invoke(self._prompt.format(q=query)).content.strip()\n","        return _retriever_call(self.base, rewritten) or []\n","\n","rewriter_retriever = RewritingRetriever(base=baseline_retriever, llm=llm_rewrite)\n","rewriter_idsafe    = IdSafeRetriever(inner=rewriter_retriever)"],"metadata":{"id":"FhpoWp_nk--i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def peek(ret, q, k=5):\n","    print(\"Q:\", q)\n","    docs = ret.invoke(q)[:k]\n","    for i, d in enumerate(docs, 1):\n","        print(f\"[{i}] id={d.metadata.get('id')} title={d.metadata.get('title')} | {d.page_content[:60]}...\")\n","\n","q_demo = \"TOPIX連動インデックスの信託財産留保額は？\"\n","print(\"— Baseline —\"); peek(baseline_retriever, q_demo)\n","print(\"\\n— MultiQuery —\"); peek(multiq_idsafe, q_demo)\n","print(\"\\n— HyDE —\"); peek(hyde_idsafe, q_demo)\n","print(\"\\n— Rewriter —\"); peek(rewriter_idsafe, q_demo)"],"metadata":{"id":"R3Mt8_vMlBOb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Baseline   :\", evaluate_with_qas(baseline_retriever, qa_items, k=5))\n","print(\"MultiQuery :\", evaluate_with_qas(multiq_idsafe,        qa_items, k=5))\n","print(\"HyDE       :\", evaluate_with_qas(hyde_idsafe,          qa_items, k=5))\n","print(\"Rewriter   :\", evaluate_with_qas(rewriter_idsafe,      qa_items, k=5))"],"metadata":{"id":"40Jy4EN4lEIE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === 1) MMR retriever: ベクトル検索の上澄みを多様化してノイズ抑制 ===\n","# Chroma → as_retriever(search_type=\"mmr\", search_kwargs={...})\n","mmr_retriever = baseline_retriever.vectorstore.as_retriever(\n","    search_type=\"mmr\",\n","    search_kwargs={\n","        \"k\": 5,        # 最終取得\n","        \"fetch_k\": 20, # 一旦広く取ってから多様化\n","        \"lambda_mult\": 0.5,  # 0=多様性重視, 1=類似度重視（0.3〜0.7で探索）\n","    }\n",")\n","\n","# Id付与（評価・RRF安定のため）\n","mmr_idsafe = IdSafeRetriever(inner=mmr_retriever)\n","\n","print(\"MMR metrics:\", evaluate_with_qas(mmr_idsafe, qa_items, k=5))"],"metadata":{"id":"WbaId1f7lL9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datetime as dt\n","from typing import Callable, List\n","\n","def _parse_date(s):\n","    try:\n","        return dt.datetime.fromisoformat(str(s)).date()\n","    except Exception:\n","        return None\n","\n","class FilteredRetriever(BaseRetriever):\n","    \"\"\"ベースRetrieverの結果（多めに fetch）に対し、メタデータで前処理フィルタ→上位kへ\"\"\"\n","    base: BaseRetriever = Field(...)\n","    k: int              = Field(default=5)\n","    fetch_k: int        = Field(default=25)\n","    filter_fn: Callable = Field(default=lambda d: True)\n","    sort_key: Callable  = Field(default=None)\n","\n","    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Any]:\n","        # まず広めに取る\n","        docs = _retriever_call(self.base, query) or []\n","        docs = list(docs)[: self.fetch_k]\n","        # フィルタ\n","        docs = [d for d in docs if self.filter_fn(d)]\n","        # ソート（例：有効日降順）\n","        if self.sort_key:\n","            docs.sort(key=self.sort_key, reverse=True)\n","        return docs[: self.k]\n","\n","# 例：public優先 + product_type=投資信託 を優先、さらに effective_date が新しい順\n","def filter_public_fund(d):\n","    m = d.metadata or {}\n","    if m.get(\"product_type\") and m.get(\"product_type\") != \"投資信託\":\n","        return False\n","    # internal も拾うなら True を返す運用でもよい（ここでは public 優先）\n","    conf = (m.get(\"confidentiality\") or \"\").lower()\n","    return conf in (\"public\", \"公開\", \"\")\n","\n","def sort_by_effective_date_desc(d):\n","    m = d.metadata or {}\n","    ed = _parse_date(m.get(\"effective_date\"))\n","    return ed or dt.date(1970,1,1)\n","\n","filtered_mmr = FilteredRetriever(\n","    base=mmr_idsafe,\n","    k=5,\n","    fetch_k=25,\n","    filter_fn=filter_public_fund,\n","    sort_key=sort_by_effective_date_desc,\n",")\n","filtered_mmr_idsafe = IdSafeRetriever(inner=filtered_mmr)\n","\n","print(\"Filtered+MMR metrics:\", evaluate_with_qas(filtered_mmr_idsafe, qa_items, k=5))"],"metadata":{"id":"e_7QfUd9nJUz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","import json\n","\n","llm_rerank = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n","\n","class LLMRerankRetriever(BaseRetriever):\n","    base: BaseRetriever = Field(...)\n","    llm:  Any           = Field(...)\n","    fetch_k: int        = Field(default=20)\n","    k: int              = Field(default=5)\n","\n","    # ★ 波括弧を {{ }} でエスケープ（JSON例・フィールド指定・コードブロック内すべて）\n","    _prompt = ChatPromptTemplate.from_messages([\n","        (\"system\",\n","         \"あなたは検索結果の再ランキングを行うアシスタントです。\"\n","         \"与えられたユーザ質問と候補ドキュメント（id, title, snippet）について、\"\n","         \"各候補が質問にどれだけ関連するかを 0.0〜1.0 のスコアで評価し、\"\n","         \"JSONで返してください。フィールドは {{\\\"id\\\": str, \\\"score\\\": float}} の配列のみ。\"),\n","        (\"human\",\n","         \"質問: {question}\\n\\n候補:\\n{candidates}\\n\\n\"\n","         \"出力: JSON配列（例: [{{\\\"id\\\":\\\"DOC001\\\",\\\"score\\\":0.83}}, ...]）のみ。\")\n","    ])\n","\n","    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Any]:\n","        pool = _retriever_call(self.base, query) or []\n","        pool = list(pool)[: self.fetch_k]\n","        if not pool:\n","            return []\n","\n","        def short(d):\n","            return {\n","                \"id\": d.metadata.get(\"id\") or \"\",\n","                \"title\": d.metadata.get(\"title\") or \"\",\n","                \"snippet\": (d.page_content or \"\")[:200]\n","            }\n","        ctext = json.dumps([short(d) for d in pool], ensure_ascii=False)\n","\n","        # ここで {question}, {candidates} を埋め込む\n","        msg = self._prompt.format_messages(question=query, candidates=ctext)\n","        out = self.llm.invoke(msg).content\n","\n","        try:\n","            scores = json.loads(out)\n","        except Exception:\n","            # JSON化失敗時は素朴に上位のまま返す\n","            return pool[: self.k]\n","\n","        score_map = {s.get(\"id\"): float(s.get(\"score\", 0.0)) for s in scores if isinstance(s, dict)}\n","        pool.sort(key=lambda d: score_map.get(d.metadata.get(\"id\"), 0.0), reverse=True)\n","        return pool[: self.k]\n","\n","llm_reranked = LLMRerankRetriever(base=filtered_mmr_idsafe, llm=llm_rerank, fetch_k=15, k=5)\n","llm_reranked_idsafe = IdSafeRetriever(inner=llm_reranked)\n","\n","print(\"LLM Rerank metrics:\", evaluate_with_qas(llm_reranked_idsafe, qa_items, k=5))"],"metadata":{"id":"aWnM02QPnL6A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 互換 import（バージョン差に備えて両方試す）\n","from langchain_classic.retrievers import ContextualCompressionRetriever\n","from langchain_classic.retrievers.document_compressors import LLMChainExtractor\n","\n","compressor = LLMChainExtractor.from_llm(llm_rerank)  # 同じ LLM でOK（温度0）\n","compressed_retriever = ContextualCompressionRetriever(\n","    base_compressor=compressor,\n","    base_retriever=llm_reranked_idsafe,  # 直前の再ランク結果をさらに圧縮\n",")\n","\n","print(\"Compressed metrics:\", evaluate_with_qas(compressed_retriever, qa_items, k=5))\n"],"metadata":{"id":"vYWlKrAlnOTf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Baseline          :\", evaluate_with_qas(baseline_retriever,     qa_items, k=5))\n","print(\"MMR               :\", evaluate_with_qas(mmr_idsafe,             qa_items, k=5))\n","print(\"Filtered+MMR      :\", evaluate_with_qas(filtered_mmr_idsafe,    qa_items, k=5))\n","print(\"LLM Rerank        :\", evaluate_with_qas(llm_reranked_idsafe,   qa_items, k=5))\n","print(\"Compressed (final):\", evaluate_with_qas(compressed_retriever,   qa_items, k=5))"],"metadata":{"id":"-fKMvpZWnc_G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_graph_retriever.transformers import ShreddingTransformer\n","from langchain_graph_retriever import GraphRetriever\n","from graph_retriever.strategies import Eager  # 公式例の戦略\n","from typing import Set\n","\n","# ---- 前提：md_docs / rc_docs のどちらかがある（Week1で生成済み）----\n","source_docs: List[Document] = md_docs if len(md_docs) > 0 else rc_docs\n","assert len(source_docs) > 0, \"md_docs/rc_docs が見つかりません。先にWeek1の分割を実行してください。\"\n","\n","source_docs = [_ensure_id(deepcopy(d)) for d in source_docs]\n","\n","# ShreddingTransformer は document を \"graph-ready\" に変換してくれます\n","# （内部でエンティティ/エッジ抽出＝トリプル抽出を行い、Docへ分解・整形）\n","shredder = ShreddingTransformer()\n","\n","graph_ready_docs: List[Document] = list(shredder.transform_documents(source_docs))\n","print(f\"[Shredding] produced {len(graph_ready_docs)} graph-ready docs\")\n","\n","# ---- 2) VectorStore 構築（Chroma）：グラフ＆元断片を同一ストアに積む公式スタイル ----\n","emb = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n","\n","# 公式の例に倣い、Shreddedドキュメントをそのまま Chroma へ\n","vector_store = Chroma.from_documents(\n","    documents=graph_ready_docs,\n","    embedding=emb,\n","    collection_name=\"wk5_graph_rag\",\n",")\n","\n","# ---- 3) エッジ（relation名）の自動検出 → GraphRetriever 構築 ----\n","# Shredding済みDocの metadata から relation（predicate）候補を拾っておく\n","def _collect_relations(docs: List[Document]) -> List[tuple]:\n","    rels: Set[str] = set()\n","    for d in docs:\n","        m = d.metadata or {}\n","        # ライブラリ実装に依存しますが、一般的に relation/predicate 相当のキーが入ります\n","        for key in (\"relation\", \"predicate\", \"edge\", \"p\"):\n","            v = m.get(key)\n","            if isinstance(v, str) and v.strip():\n","                rels.add(v.strip())\n","    # GraphRetriever は (edge_label, relation_key) のタプル列を受け取る仕様\n","    # 公式デモでは (\"habitat\", \"habitat\") のように同名で与えています。\n","    if not rels:\n","        # もし抽出できない/ゼロ件でも動くように保険で代表的な金融用語をいくつか置く\n","        rels = {\"信託報酬\", \"カットオフタイム\", \"信託財産留保額\", \"販売勧誘時間\"}\n","    return [(r, r) for r in sorted(rels)]\n","\n","edges = _collect_relations(graph_ready_docs)\n","print(f\"[Edges] relations detected: {len(edges)} -> {edges[:8]}{' ...' if len(edges)>8 else ''}\")\n","\n","# GraphRetriever（トラバーサル戦略は公式例の Eager）\n","traversal_retriever = GraphRetriever(\n","    store=vector_store,\n","    edges=edges,                          # 関係の種類（predicate名）を与える\n","    strategy=Eager(k=5, start_k=1, max_depth=2),\n",")\n","\n","# ---- 4) 動作確認 ----\n","def peek(ret, q, k=5):\n","    print(\"Q:\", q)\n","    docs = ret.invoke(q)[:k]\n","    for i, d in enumerate(docs, 1):\n","        print(f\"[{i}] id={d.metadata.get('id')} title={d.metadata.get('title')} | {d.page_content[:80]}...\")\n","\n","q_demo = \"サクラ短期国債ファンドの信託報酬は？\"\n","peek(traversal_retriever, q_demo)\n"],"metadata":{"id":"JuddZILmnuCE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 既存の評価関数を流用（AnswerHit/SourceHit/Noise）\n","print(\"GraphRAG (GraphRetriever):\", evaluate_with_qas(traversal_retriever, qa_items, k=5))"],"metadata":{"id":"3Ru0dkdxmPyH"},"execution_count":null,"outputs":[]}]}